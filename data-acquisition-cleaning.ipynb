{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Cleaning\n",
    "\n",
    "This project relys on two different sources of data: United States' Bureau of Transport Statistics, and National Climatic Data Center. The fisrt source can only be accessed via its website, while the second provides an API with an extensive guide for utilising it.\n",
    "\n",
    "Bureau of Transport Statistics provides extensive statistics on various forms of transportation, including airline performance data which consists of various figures related to departure and arrival delays. Data related to any year back to 1987 can be accessed in monthly chunks. However, given that no API is provided, the relevant data should be downloaded via a menu system by selecting all the required features.\n",
    "\n",
    "National Climatic Data Center offers historical weather data for a host of weather stations inside and outside the United States. The provided API works through a request link that can be constructed according to one's needs for any time period depending on availability. Offered features include hourly weather condition, liquid precipitation, air temperature and many more.\n",
    "\n",
    "Both of these sources provide the requested data in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impoting the necessary python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import requests as req\n",
    "import io\n",
    "from datetime import datetime\n",
    "from meteostat import Hourly\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flight Data\n",
    "As mentioned above, the Bureau of Transport Statistics provides a menu system for data selection and download, which was saved to disk for the period starting 1st January 2017 and ending 31st December 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list containing all the csv files and sorting them alphabetically\n",
    "csv_list = glob.glob('../data-sources/capstone-project/flight-data/[0-9][0-9].csv')\n",
    "csv_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains information about delays which are aggregated under National Aviation System delays, which contains delays related to less than severe weather events. The only information available about the weather portion of this aggregate value is a monthly percentage given on a national level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary containing the monthly weather share of delays labeled as National Aviation System delays\n",
    "weather_share_nas = {'2017-01': 0.6617,\n",
    "                     '2017-02': 0.6814,\n",
    "                     '2017-03': 0.6475,\n",
    "                     '2017-04': 0.5684,\n",
    "                     '2017-05': 0.5320,\n",
    "                     '2017-06': 0.6302,\n",
    "                     '2017-07': 0.7192,\n",
    "                     '2017-08': 0.7191,\n",
    "                     '2017-09': 0.6112,\n",
    "                     '2017-10': 0.5916,\n",
    "                     '2017-11': 0.6567,\n",
    "                     '2017-12': 0.5707,\n",
    "                     '2018-01': 0.6056,\n",
    "                     '2018-02': 0.6057,\n",
    "                     '2018-03': 0.5891,\n",
    "                     '2018-04': 0.6408,\n",
    "                     '2018-05': 0.7559,\n",
    "                     '2018-06': 0.6445,\n",
    "                     '2018-07': 0.7273,\n",
    "                     '2018-08': 0.7956,\n",
    "                     '2018-09': 0.6346,\n",
    "                     '2018-10': 0.6169,\n",
    "                     '2018-11': 0.6599,\n",
    "                     '2018-12': 0.6391,\n",
    "                     '2019-01': 0.6986,\n",
    "                     '2019-02': 0.6886,\n",
    "                     '2019-03': 0.6188,\n",
    "                     '2019-04': 0.7336,\n",
    "                     '2019-05': 0.7774,\n",
    "                     '2019-06': 0.7497,\n",
    "                     '2019-07': 0.7903,\n",
    "                     '2019-08': 0.7390,\n",
    "                     '2019-09': 0.5391,\n",
    "                     '2019-10': 0.5939,\n",
    "                     '2019-11': 0.6203,\n",
    "                     '2019-12': 0.6742}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the first csv file, changing the column names to lower case, renaming the weather_delay to\n",
    "#extreme_weather_delay for greater accuracy and extracting the weather share of NAS delays\n",
    "flight_set = pd.read_csv(csv_list[0])\n",
    "flight_set.columns = flight_set.columns.str.lower()\n",
    "flight_set.rename(columns={'weather_delay': 'extreme_weather_delay'}, inplace=True)\n",
    "flight_set['weather_delay'] = flight_set['nas_delay'] * weather_share_nas[list(weather_share_nas.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a loop to load the remaining files\n",
    "for i, file in enumerate(csv_list[1:]):\n",
    "    #loading the remaining files and concatenating the contents one by one into a single dataframe\n",
    "    next_flight_set = pd.read_csv(file)\n",
    "    next_flight_set.columns = next_flight_set.columns.str.lower()\n",
    "    next_flight_set.rename(columns={'weather_delay': 'extreme_weather_delay'}, inplace=True)\n",
    "    next_flight_set['weather_delay'] = next_flight_set['nas_delay'] * \\\n",
    "                                       weather_share_nas[list(weather_share_nas.keys())[i]]\n",
    "    flight_set = pd.concat([flight_set, next_flight_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the top twenty airports of origin and forming a new dataframe containing the data related to\n",
    "#these airports only\n",
    "top_twenty_origins = list(flight_set['origin'].value_counts().sort_values(ascending=False).head(20).index)\n",
    "top_twenty_airports = flight_set[flight_set['origin'].isin(top_twenty_origins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the schedulled departure time into a string\n",
    "top_twenty_airports['crs_dep_time'] = top_twenty_airports['crs_dep_time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to transform the schedulled departure time to a consistant 4-digit format\n",
    "def correct_time(cell):\n",
    "    if len(cell) == 3:\n",
    "        return '0'+cell\n",
    "    elif len(cell) == 2:\n",
    "        return '00'+cell\n",
    "    elif len(cell) == 1:\n",
    "        return '000'+cell\n",
    "    else:\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the above function to correct the formatting\n",
    "top_twenty_airports['crs_dep_time'] = top_twenty_airports['crs_dep_time'].apply(correct_time)\n",
    "#converting the schedulled departure time to hh:mm:ss format\n",
    "top_twenty_airports['crs_dep_time'] = top_twenty_airports['crs_dep_time'].apply(lambda x: x[0: 2]+':'+x[2:]+':00')\n",
    "#combining the schedulled departure time with the departure date in order to create a single timestamp\n",
    "top_twenty_airports['dep_timestamp'] = pd.to_datetime(top_twenty_airports['fl_date'] \\\n",
    "                                                      +' '+top_twenty_airports['crs_dep_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the that are no longer required and sorting the dataframe by origin\n",
    "top_twenty_airports.drop(['unnamed: 25', 'fl_date', 'crs_dep_time', 'dep_time'], axis=1, inplace=True)\n",
    "top_twenty_airports.sort_values('origin', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the order inwhich the columns appear in the dataframe\n",
    "top_twenty_airports = top_twenty_airports[['dep_timestamp', 'op_unique_carrier', 'tail_num', 'origin',\n",
    "                                           'origin_city_name', 'dest', 'dest_city_name', 'dep_delay_new',\n",
    "                                           'wheels_off', 'wheels_on', 'taxi_in', 'arr_delay_new', 'cancelled',\n",
    "                                           'cancellation_code', 'diverted', 'air_time', 'distance', 'carrier_delay',\n",
    "                                           'extreme_weather_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "                                           'late_aircraft_delay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the missing values in the various delay columns to zero\n",
    "top_twenty_airports['carrier_delay'].fillna(0, inplace=True)\n",
    "top_twenty_airports['extreme_weather_delay'].fillna(0, inplace=True)\n",
    "top_twenty_airports['weather_delay'].fillna(0, inplace=True)\n",
    "top_twenty_airports['nas_delay'].fillna(0, inplace=True)\n",
    "top_twenty_airports['security_delay'].fillna(0, inplace=True)\n",
    "top_twenty_airports['late_aircraft_delay'].fillna(0, inplace=True)\n",
    "\n",
    "#writing the dataframe into a csv file\n",
    "top_twenty_airports.to_csv('../data-sources/capstone-project/flight-data/top-twenty-airports.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data\n",
    "Accessing the API provided by the National Climatic Data Center is possible through a request link which should contain the name of the database, target weather station, start and end time, required features and so on. This can be done by following the instructions available on the website. Although this data is extensive, there are some missing values, especially when it comes to information about weather condition. To fill this gap, I decided to use the Meteostat Python library as a secondary source of historic weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the url template for NCDC's API\n",
    "url_template = 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-hourly&stations={}&includeStationName=1&includeStationLocation=1&startDate=2016-12-31&endDate=2020-01-01&dataTypes=TMP,VIS,WND,CIG,DEW,SLP,AA1&units=metric&format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the start and end parameters to be used by meteostat module\n",
    "start = datetime(2016, 12, 31)\n",
    "end = datetime(2020, 1, 1, 23, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary containing the station codes related to the top twenty airports\n",
    "station_dict = {'DFW': '72259003927',\n",
    "                'DCA': '72405013743',\n",
    "                'ORD': '72530094846',\n",
    "                'SFO': '72494023234',\n",
    "                'PHX': '72278023183',\n",
    "                'LAS': '72386023169',\n",
    "                'JFK': '74486094789',\n",
    "                'LGA': '72503014732',\n",
    "                'MSP': '72658014922',\n",
    "                'IAH': '72243012960',\n",
    "                'SEA': '72793024233',\n",
    "                'SLC': '72572024127',\n",
    "                'LAX': '72295023174',\n",
    "                'DEN': '72565003017',\n",
    "                'ATL': '72219013874',\n",
    "                'EWR': '72502014734',\n",
    "                'MCO': '72205012815',\n",
    "                'BOS': '72509014739',\n",
    "                'DTW': '72537094847',\n",
    "                'CLT': '72314013881'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to extract feature values from the raw data\n",
    "def value_extractor(cell, position):\n",
    "    try:\n",
    "        return int(cell.split(',')[position])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind direction data is give in degrees. This is a circular scale in which smaller and larger values don't have the same meaning as commonly accepted. Therefore, I decided to encode the 360 degree scale into eight different categories for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definging a function to encode wind direction figures\n",
    "def wind_encoder(cell):\n",
    "    if cell >= 22.5 and cell < 67.5:\n",
    "        return 'north-east'\n",
    "    elif cell >= 67.5 and cell < 112.5:\n",
    "        return 'east'\n",
    "    elif cell >= 112.5 and cell < 157.5:\n",
    "        return 'south-east'\n",
    "    elif cell >= 157.5 and cell < 202.5:\n",
    "        return 'south'\n",
    "    elif cell >= 202.5 and cell < 247.5:\n",
    "        return 'south-west'\n",
    "    elif cell >= 247.5 and cell < 292.5:\n",
    "        return 'west'\n",
    "    elif cell >= 247.5 and cell < 337.5:\n",
    "        return 'north-west'\n",
    "    else:\n",
    "        return 'north'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a loop to download and clean the data related to each station one by one\n",
    "for key in list(station_dict.keys()):\n",
    "    #sending the request via the API url, reading the downloaded csv file into a pandas dataframe and changing\n",
    "    #column names to lower case\n",
    "    weather_file = req.get(url_template.format(station_dict[key]))\n",
    "    weather = pd.read_csv(io.StringIO(weather_file.content.decode('utf-8')))\n",
    "    weather.columns = weather.columns.str.lower()\n",
    "\n",
    "    #converting the time column to pandas datetime and setting that column as dataframe index\n",
    "    weather['date'] = pd.to_datetime(weather['date'])\n",
    "    weather.set_index('date', drop=True, inplace=True)\n",
    "\n",
    "    #dropping the unnecessary columns\n",
    "    weather.drop(['source', 'report_type', 'call_sign', 'quality_control'], axis=1, inplace=True)\n",
    "\n",
    "    #replacing the code column with the current key and re-ordering the columns\n",
    "    weather['code'] = key\n",
    "    weather = weather[['station', 'name', 'code', 'latitude', 'longitude', 'elevation', 'aa1', 'cig', 'dew',\n",
    "                                    'slp', 'tmp', 'vis', 'wnd']]\n",
    "\n",
    "    #setting up the conditions for the correct discovery of the hourly observations which depends on the station\n",
    "    if key != 'DTW':\n",
    "        weather.drop(weather[weather.index.minute != weather.index.minute[3]].index, inplace=True)\n",
    "    else:\n",
    "        weather.drop(weather[weather.index.minute != weather.index.minute[4]].index, inplace=True)\n",
    "\n",
    "    #resetting the index to a numeric value\n",
    "    weather.reset_index(inplace=True)\n",
    "\n",
    "    #dropping repeat observations from EWR station which has unique characteristics\n",
    "    if key == 'EWR':\n",
    "        weather.drop(weather[(weather['slp'].str.contains('99999')) &\n",
    "                                        (weather['dew'] == '+0167,1')].index, inplace=True)\n",
    "\n",
    "    #dropping repeat observations with a specific pattern\n",
    "    weather.drop(weather[(weather['cig'].str.contains('99999')) & (weather['dew'].str.contains('9999')) &\n",
    "                                        (weather['slp'].str.contains('99999')) &\n",
    "                                        (weather['tmp'].str.contains('9999')) &\n",
    "                                        (weather['vis'].str.contains('999999'))].index, inplace=True)\n",
    "\n",
    "    #creating a list of missing timestamps\n",
    "    missing_hours = list(set(pd.date_range(list(weather['date'])[0], list(weather['date'])[-1], \n",
    "                                                               periods=26328)) - set(weather['date']))\n",
    "\n",
    "    #setting up a loop to add these missing onservations to the dataframe and setting their\n",
    "    #features to known or missing values depending on the specific feature\n",
    "    for hour in missing_hours:\n",
    "        new_hour = {'date': hour,\n",
    "                    'station': weather['station'][0],\n",
    "                    'name': weather['name'][0],\n",
    "                    'latitude': weather['latitude'][0],\n",
    "                    'longitude': weather['longitude'][0],\n",
    "                    'elevation': weather['elevation'][0],\n",
    "                    'aa1': np.nan,\n",
    "                    'cig': np.nan,\n",
    "                    'dew': np.nan,\n",
    "                    'slp': np.nan,\n",
    "                    'tmp': np.nan,\n",
    "                    'vis': np.nan,\n",
    "                    'wnd': np.nan,\n",
    "                    'wnd_dir': np.nan}\n",
    "\n",
    "        weather = weather.append(new_hour, ignore_index=True)\n",
    "\n",
    "    #sorting the dataframe according to time and resetting the index\n",
    "    weather.sort_values('date', inplace=True)\n",
    "    weather.set_index('date', drop=True, inplace=True)\n",
    "\n",
    "    #using the meteostat module to download further data related to weather condition\n",
    "    add_weather = Hourly(station_dict[key][:5], start, end)\n",
    "    add_weather = add_weather.normalize()\n",
    "    add_weather = add_weather.fetch()\n",
    "    #converting observations with code zero to missing values and adding the to the data\n",
    "    #downloaded through NCDC's API\n",
    "    add_weather.loc[add_weather[add_weather['coco'] == 0.0].index, 'coco'] = np.nan\n",
    "    add_weather.set_index(weather.index, drop=True, inplace=True)\n",
    "    weather['au1'] = add_weather['coco']\n",
    "\n",
    "    #extracting precipitation figures, converting to millimeters, and turning missing data into NaNs\n",
    "    weather['aa1'] = weather['aa1'].apply(value_extractor, args=(1,))\n",
    "    weather['aa1'] = weather['aa1'].apply(lambda x: x/10)\n",
    "    weather.loc[weather[weather['aa1'] == 999.9].index, 'aa1'] = np.nan\n",
    "\n",
    "    #extracting cloud alititude figures and converting missing data into NaNs\n",
    "    weather['cig'] = weather['cig'].apply(value_extractor, args=(0,))\n",
    "    weather.loc[weather[weather['cig'] == 99999].index, 'cig'] = np.nan\n",
    "\n",
    "    #extracting dew point figures, converting to degrees celsius, and turning missing data into NaNs\n",
    "    weather['dew'] = weather['dew'].apply(value_extractor, args=(0,))\n",
    "    weather['dew'] = weather['dew'].apply(lambda x: x/10)\n",
    "    weather.loc[weather[weather['dew'] == 999.9].index, 'dew'] = np.nan\n",
    "\n",
    "    #extracting pressure figures and turning missing data into NaNs\n",
    "    weather['slp'] = weather['slp'].apply(value_extractor, args=(0,))\n",
    "    weather.loc[weather[weather['slp'] == 99999].index, 'slp'] = np.nan\n",
    "\n",
    "    #extracting air temperature figures, converting to degrees celsius, and turning missing data into NaNs\n",
    "    weather['tmp'] = weather['tmp'].apply(value_extractor, args=(0,))\n",
    "    weather['tmp'] = weather['tmp'].apply(lambda x: x/10)\n",
    "    weather.loc[weather[weather['tmp'] == 999.9].index, 'tmp'] = np.nan\n",
    "\n",
    "    #extracting visibility figures and turning missing data into NaNs\n",
    "    weather['vis'] = weather['vis'].apply(value_extractor, args=(0,))\n",
    "    weather.loc[weather[weather['vis'] == 999999].index, 'vis'] = np.nan\n",
    "\n",
    "    #extracting wind direction figures and replacing missing data with NaNs\n",
    "    weather['wnd_dir'] = weather['wnd'].apply(value_extractor, args=(0,))\n",
    "    weather.loc[weather[weather['wnd_dir'] == 999].index, 'wnd_dir'] = np.nan\n",
    "\n",
    "    #extracting wind speed figures, converting into meters per second, and turning missing data into NaNs\n",
    "    weather['wnd'] = weather['wnd'].apply(value_extractor, args=(3,))\n",
    "    weather['wnd'] = weather['wnd'].apply(lambda x: x/10)\n",
    "    weather.loc[weather[weather['wnd'] == 999.9].index, 'wnd'] = np.nan\n",
    "\n",
    "    #resetting dataframe index to numerical values\n",
    "    weather.reset_index(inplace=True)\n",
    "\n",
    "    #creating a new dataframe containing only the continuous features with index equal to the primary dataframe\n",
    "    weather_partial = weather[['aa1', 'cig', 'dew', 'slp', 'tmp', 'vis', 'wnd', 'wnd_dir']]\n",
    "    weather_partial.set_index(weather.index, inplace=True)\n",
    "\n",
    "    #instantiating a StandardScaler and standardising this partial data\n",
    "    scaler = StandardScaler()\n",
    "    weather_partial_std = pd.DataFrame(scaler.fit_transform(weather_partial), index=weather.index,\n",
    "                                                                columns=weather_partial.columns)\n",
    "\n",
    "    #instantiating a KNNImputer in order to impute the missing values in continuous features\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    weather_partial_std = pd.DataFrame(imputer.fit_transform(weather_partial_std),\n",
    "                                       columns=weather_partial_std.columns)\n",
    "\n",
    "    #inverse tranforming the standardised data in order to go back to original units\n",
    "    weather_partial = pd.DataFrame(scaler.inverse_transform(weather_partial_std), index=weather.index,\n",
    "                                                                columns=weather_partial_std.columns)\n",
    "\n",
    "    #replacing the continuous features in the primary dataframe with this newly imputed data\n",
    "    weather[['aa1', 'cig', 'dew', 'slp', 'tmp', 'vis', 'wnd', 'wnd_dir']] = \\\n",
    "                                    np.round(weather_partial[['aa1', 'cig', 'dew', 'slp', 'tmp',\n",
    "                                                                'vis', 'wnd', 'wnd_dir']], 1)\n",
    "\n",
    "    #creating another subset containing the continuous features plus the categorical weather condition data\n",
    "    features = weather[['aa1', 'cig', 'dew', 'slp', 'tmp', 'vis', 'wnd', 'wnd_dir', 'au1']]\n",
    "\n",
    "    #dividing thid subset based on available and missing data in weather condition column\n",
    "    X = features[features['au1'].notna()]\n",
    "    X_predict = features[features['au1'].isnull()]\n",
    "\n",
    "    #separating the predictor and target variables\n",
    "    y = X.pop('au1')\n",
    "    X_predict.pop('au1')\n",
    "\n",
    "    #dividing the data with no missing weather condition figures into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    #standardising the train, test and the predict sets\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    X_predict = pd.DataFrame(scaler.transform(X_predict), index=X_predict.index, columns=X_predict.columns)\n",
    "\n",
    "    #instantiating a KNeighborsClassifier model in order to predict the missing weather condition data\n",
    "    model_knn = KNeighborsClassifier(n_jobs=-2, n_neighbors=6, metric='manhattan', weights='distance')\n",
    "    model_knn.fit(X_train, y_train)\n",
    "\n",
    "    #inserting the predicted weather condition figures into the primary dataframe\n",
    "    weather.loc[X_predict.index, 'au1'] = pd.Series(model_knn.predict(X_predict), index=X_predict.index)\n",
    "    \n",
    "    #applying the function to the wind direction figures\n",
    "    weather['wnd_dir'] = weather['wnd_dir'].apply(wind_encoder)\n",
    "\n",
    "    #turning the date column into the dataframe index\n",
    "    weather.set_index('date', drop=True, inplace=True)\n",
    "\n",
    "    #writing the weather data to csv files\n",
    "    weather.to_csv(f'../data-sources/capstone-project/weather-data/{key}-weather.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Flight and Weather Data\n",
    "Once the two sets of data is collated and cleaned, they have to be joined together in a single set. This can be done separately for each individual departure airport and then gathered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the flight data into a dataframe\n",
    "flights = pd.read_csv('../data-sources/capstone-project/flight-data/top-twenty-airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dep_timestamp column into pandas datetime, renaming it and sorting\n",
    "#the dataframe according to this column\n",
    "flights['dep_timestamp'] = pd.to_datetime(flights['dep_timestamp'])\n",
    "flights.rename(columns={'dep_timestamp': 'time'}, inplace=True)\n",
    "flights.sort_values('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of csv files containing weather data for all origin airports\n",
    "weather_csv_list = glob.glob('../data-sources/capstone-project/weather-data/[A-Z][A-Z][A-Z]-weather.csv')\n",
    "weather_csv_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a loop to join the flight data and the weather data from different origins\n",
    "for file in weather_csv_list:\n",
    "\n",
    "    #loading the first weather csv, converting the date column into pandas datetime and renaming it\n",
    "    weather = pd.read_csv(file)\n",
    "    weather['date'] = pd.to_datetime(weather['date'])\n",
    "    weather.rename(columns={'date': 'time'}, inplace=True)\n",
    "\n",
    "    #subsetting the flight data using the origin airport from the weather data\n",
    "    airport = flights[flights['origin'] == weather.loc[weather.index[0], 'code']]\n",
    "\n",
    "    #joining the two dataframe on time according to the nearest values and resetting the index to time\n",
    "    airport_weather = pd.merge_asof(airport, weather, on='time')\n",
    "    airport_weather.set_index('time', drop=True, inplace=True)\n",
    "\n",
    "    #writing the combined dataframe to a csv file\n",
    "    airport_weather.to_csv(f\"../data-sources/capstone-project/agg-data/{weather.loc[weather.index[0], 'code']}-airport.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of all the csv files containing the combined flight and eather data for each origin airport\n",
    "airport_csv_list = glob.glob('../data-sources/capstone-project/agg-data/[A-Z][A-Z][A-Z]-airport.csv')\n",
    "airport_csv_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lading the first file into a dataframe\n",
    "airport_weather_agg = pd.read_csv(airport_csv_list[0], infer_datetime_format=True, index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping over the remaining files, loading them into a dataframe and concatenating them\n",
    "#with the dataframe loaded above\n",
    "for file in airport_csv_list[1:]:\n",
    "    next_airport_weather_agg = pd.read_csv(file, infer_datetime_format=True, index_col='time')\n",
    "    airport_weather_agg = pd.concat([airport_weather_agg, next_airport_weather_agg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all the unwanted features\n",
    "airport_weather_agg.drop(['station', 'origin_city_name', 'dest_city_name', 'name', 'code', 'taxi_in',\n",
    "                                    'wheels_on', 'wheels_off'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming some of the columns to more usable names\n",
    "airport_weather_agg.rename(columns={'op_unique_carrier': 'carrier',\n",
    "                                    'dest': 'destination',\n",
    "                                    'dep_delay_new': 'dep_delay',\n",
    "                                    'arr_delay_new': 'arr_delay',\n",
    "                                    'aa1': 'precipitation',\n",
    "                                    'au1': 'condition',\n",
    "                                    'cig': 'cloud_base',\n",
    "                                    'dew': 'dew_temp',\n",
    "                                    'slp': 'pressure',\n",
    "                                    'tmp': 'air_temp',\n",
    "                                    'vis': 'visibility',\n",
    "                                    'wnd': 'wind_speed',\n",
    "                                    'wnd_dir': 'wind_direction'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordering the columns\n",
    "airport_weather_agg = airport_weather_agg[['origin', 'latitude', 'longitude', 'elevation', 'carrier', 'tail_num',\n",
    "                                           'precipitation', 'condition', 'cloud_base', 'dew_temp', 'pressure',\n",
    "                                           'air_temp', 'visibility', 'wind_speed', 'wind_direction', 'dep_delay',\n",
    "                                           'carrier_delay', 'extreme_weather_delay', 'weather_delay', 'nas_delay',\n",
    "                                           'security_delay', 'late_aircraft_delay', 'cancelled', 'cancellation_code',\n",
    "                                           'diverted', 'destination', 'arr_delay', 'air_time', 'distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the dataframe by its datetime index\n",
    "airport_weather_agg.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating different combinations of the data\n",
    "geographical_features = airport_weather_agg[['origin', 'latitude', 'longitude', 'elevation', 'destination']]\n",
    "carrier_features = airport_weather_agg[['carrier', 'tail_num', 'diverted', 'air_time', 'distance']]\n",
    "core_features = airport_weather_agg[['origin', 'precipitation', 'condition', 'cloud_base', 'dew_temp', 'pressure',\n",
    "                                     'air_temp', 'visibility', 'wind_speed', 'wind_direction', 'dep_delay',\n",
    "                                     'carrier_delay', 'extreme_weather_delay', 'weather_delay', 'nas_delay',\n",
    "                                     'security_delay', 'late_aircraft_delay', 'cancelled', 'cancellation_code',\n",
    "                                     'arr_delay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the different combinations to csv files\n",
    "airport_weather_agg.to_csv('../data-sources/capstone-project/agg-data/airport-weather-agg.csv')\n",
    "geographical_features.to_csv('../data-sources/capstone-project/agg-data/geographical_features.csv')\n",
    "carrier_features.to_csv('../data-sources/capstone-project/agg-data/carrier_features.csv')\n",
    "core_features.to_csv('../data-sources/capstone-project/agg-data/core_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
